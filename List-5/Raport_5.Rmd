---
title: "Raport_5"
author: "Magdalena Wawrzyniak"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Zadanie 1

Przeprowadzamy regresję liniową dla danych, w których zmienną objaśnianą jest poziom satysfakcji, a zmiennymi objaśniającymi są wiek, ciężkość choroby, poziom lęku. Wgrywamy dane a następnie tworzymy model przy pomocy funkcji `lm`.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggpubr)
library(ggplot2)
library(MASS)
library(carData)
library(car)
library(leaps)

# Dane do zadania 1-4

dane14 <- read.table("CH06PR15.txt")
colnames(dane14) = c("Wiek", "Ciężkość choroby", "Poziom lęku", "Poziom satysfakcji")
Satysfakcja <- dane14$`Poziom satysfakcji`
Wiek <- dane14$Wiek
Ciężkość_choroby <- dane14$`Ciężkość choroby`
Poziom_lęku <- dane14$`Poziom lęku`

#model regresji zad 1

model_satysfakcji <- lm(Satysfakcja ~ Wiek + Ciężkość_choroby + Poziom_lęku)
sumar <- summary(model_satysfakcji)
anova_sat <- anova(model_satysfakcji)
# współczynniki równania regresji
coef_1 = round(coefficients(model_satysfakcji), 4)
```

Dopasowane równanie regresji liniowej jest postaci:

$\hat{Y} =$ `r abs(coef_1[1])` - `r abs(coef_1[2])`$X_1$ + `r abs(coef_1[3])`$X_2$ + `r abs(coef_1[4])`$X_3$

**Współczynnik determinacji** $R^2 =$ `r summary(model_satysfakcji)$r.squared`, stąd wiemy, że model mógłby być lepiej dopasowany do naszych danych.

Testujemy nasz model i sprawdzamy istotność trzech zmiennych objaśniających.

$H_0: \beta_1 = \beta_2 = \beta_3 = 0$ vs $H_1:  \beta_1 \neq 0$ v $\beta_2 \neq 0$ v $\beta_3 \neq 0$
§

**Wartość statystyki testowej** $F =$ `r summary(model_satysfakcji)$fstatistic[1]`,  na **poziomie istotności** 0.95 i **stopniami swobody** `r summary(model_satysfakcji)$fstatistic[2:3]` oraz **p-wartością** bliską 0.

**Wniosek:** Odrzucamy hipotezę zerową i przyjmujemy hipotezę alternatywną, czyli zmienna objaśniana zależy od przynajmniej od jednej zmiennej niezależnej.


## Zadanie 2

Przedziału ufności na poziomie istotności 0.95 dla zmiennych z modelu:

```{r}
confint(model_satysfakcji)
```

Stworzymy modele regresji prostej osobno dla wieku, cięzkości choroby oraz poziomu lęku, a następnie przeprowadzimy testy.

### Wiek

```{r echo=FALSE}
model_wiek <- lm(Satysfakcja~Wiek)
summ_wiek = summary(model_wiek)
r_wiek = c(summ_wiek$fstatistic[1], summ_wiek$fstatistic[2:3], round(summ_wiek$coefficients[2,4], 4))
coef_wiek = round(coefficients(model_wiek), 4)
```

Dopasowane równanie regresji liniowej prostej:

$\hat{Y} =$ `r abs(coef_wiek[1])` - `r abs(coef_wiek[2])`$X_1$ 

Testujemy nasz model i sprawdzamy istotność trzech zmiennych objaśniających.

$H_0: \beta_1 = 0$ vs $H_1:  \beta_1 \neq 0$

Dostajemy następującą statystykę testową F `r r_wiek[1]` z `r r_wiek[2:3]` stopniami swobody oraz p-wartością równą około `r r_wiek[4]`.

**Wniosek:** Odrzucamy hipotezę zerową i przyjmujemy hipotezę alternatywną. Poziom satysfakcji zależy od od wieku.

### Ciężkość choroby

```{r echo=FALSE}
model_cc <- lm(Satysfakcja~Ciężkość_choroby)
summ_cc = summary(model_cc)
r_cc = c(summ_cc$fstatistic[1], summ_cc$fstatistic[2:3], round(summ_cc$coefficients[2,4], 4))
coef_cc = round(coefficients(model_cc), 4)
```


Dopasowane równanie regresji liniowej prostej:

$\hat{Y} =$ `r abs(coef_cc[1])` - `r abs(coef_cc[2])`$X_1$ 

Testujemy nasz model i sprawdzamy istotność trzech zmiennych objaśniających.

$H_0: \beta_1 = 0$ vs $H_1:  \beta_1 \neq 0$

Dostajemy następującą statystykę testową F `r r_cc[1]` z `r r_cc[2:3]` stopniami swobody oraz p-wartością równą około `r r_cc[4]`.

**Wniosek:** Odrzucamy hipotezę zerową i przyjmujemy hipotezę alternatywną. Poziom satysfakcji zależy od od ciężkości choroby.

### Poziom lęku

```{r echo=FALSE}
model_pl <- lm(Satysfakcja~Poziom_lęku)
summ_pl = summary(model_pl)
r_pl = c(summ_pl$fstatistic[1], summ_pl$fstatistic[2:3], round(summ_pl$coefficients[2,4], 4))
coef_pl = round(coefficients(model_pl), 4)
```

Dopasowane równanie regresji liniowej prostej:

$\hat{Y} =$ `r abs(coef_pl[1])` - `r abs(coef_pl[2])`$X_1$ 

Testujemy nasz model i sprawdzamy istotność trzech zmiennych objaśniających.

$H_0: \beta_1 = 0$ vs $H_1:  \beta_1 \neq 0$

Dostajemy następującą statystykę testową F `r r_pl[1]` z `r r_pl[2:3]` stopniami swobody oraz p-wartością równą około `r r_pl[4]`.

**Wniosek:** Odrzucamy hipotezę zerową i przyjmujemy hipotezę alternatywną. Poziom satysfakcji zależy od od poziomu lęku.

## Zadanie 3

**Wykres reszt**

```{r echo=FALSE}
par(mfrow=c(1,3))
plot(Wiek, summ_wiek$residuals, ylim = -0.55:0.55, ylab = 'Residua')
abline(h=0, lwd=1.5, col='red')
plot(Ciężkość_choroby, summ_cc$residuals, ylim = -0.55:0.55, ylab = 'Residua')
abline(h=0, lwd=1.5, col='red')
plot(Poziom_lęku, summ_pl$residuals, ylim = -0.55:0.55, ylab = 'Residua')
abline(h=0, lwd=1.5, col='red')

```

**Wniosek:** Nie widać na wykresie żadnych nietypowych wzorców ani wartości odstających.

## Zadanie 4

Sprawdzimy teraz czy residua mają rozkład w przybliżeniu normalny korzystając funkcji kwantylowo-kwantylowej (Q-Q plot) oraz z testu Shapiro-Wilka, gdzie:

$H_0:$ Próba pochodzi z populacji o rozkładzie normalnym.

$H_1:$ Próba nie pochodzi z populacji o rozkładzie normalnym.

```{r echo=FALSE, message=FALSE, warning=FALSE}
shapiro.test(summ_wiek$residuals)
ggqqplot(summ_wiek$residuals)
```

**Wniosek:** Odrzucamy hipotezę zerową i przyjmujemy alternatywną, czyli residua nie pochodzą z rozkładu normalnego. Patrząc na wykres kwantylowo-kwantylowy możemy stwierdzić, że nasze dane mają bardziej odstające wartości, niż można by się spodziewać, gdyby naprawdę pochodziły z rozkładu normalnego.

```{r echo=FALSE, message=FALSE, warning=FALSE}
shapiro.test(summ_cc$residuals)
ggqqplot(summ_cc$residuals)
```

**Wniosek:** Nie mamy podstaw do odrzucenia hipotezy zerowej, residua mogą mieć rozkład normalny. Na wykresie dane oscylują w pobliżu prostej, co wskazuje na to, że mogą pochodzić z rozkładu normalnego.

```{r echo=FALSE, message=FALSE, warning=FALSE}
shapiro.test(summ_pl$residuals)
ggqqplot(summ_pl$residuals)
```

**Wniosek:** Nie mamy podstaw do odrzucenia hipotezy zerowej, residua mogą mieć rozkład normalny. Na wykresie dane oscylują w pobliżu prostej, co wskazuje na to, że mogą pochodzić z rozkładu normalnego.

## Zadanie 5

```{r echo=FALSE}
# Wgranie danych do zadań 5-8.
dane <- read.table("csdata.txt")
colnames(dane) = c("ID", "GPA", "HSM", "HSS", "HSE", "SATM", "SATV", "SEX")

# Y = GPA
GPA <- dane$GPA

# Model pełen (F) z HSM, HSS, HSE, SATM, SATV
# Model zredukowany (R) bez SATM, SATV

# Zapisujemy zmienne wykorzystywane do modeli (F) i (R) 
HSM <- dane$HSM
HSS <- dane$HSS
HSE <- dane$HSE

SATM <- dane$SATM
SATV <- dane$SATV
SEX <- dane$SEX
```

Modelem pełnym (ozn. model_F)  będziemy nazywali model z pięcioma zmiennymi objaśniającymi ($HSM$, $HSS$, $HSE$, $SATM$, $SATV$), a modelem zredukowanym (ozn. model_R) model z 3 zmiennymi ($HSM$, $HSS$, $HSE$). Niech $n$ oznacza liczebność prób, a $p$ ilość parametrów modelu.

### a)
 
```{r echo=FALSE}
# Zapisujemy n - liczność próby, p - ilość zmiennych w modelu (z interseptem)
n <- length(GPA)
p_F <- 6
p_R <- 4

# Stworzenie modeli (R) i (F)
model_R <- lm(GPA ~ HSM + HSS + HSE) # model zredukowany
model_F <- lm(GPA ~ SATM + SATV + HSM + HSS + HSE) # model pełen

# Estymator Y (GPA)
est_GPA_R <- fitted(model_R)
est_GPA_F <- fitted(model_F)
```

Konstruujemy ręcznie statystykę F, wykorzystując różnicę między SSE dla modeli zredukowanego i pełnego, aby przetestować hipotezy:

$H_0: \beta_4 = \beta_5 = 0$ vs $H_1:  \beta_4 \neq 0$ v $\beta_5 \neq 0$.

Wzór, którego używamy do podania statystyki F wygląda następująco:

$$F = \frac{SSE_R - SSE_F}{(dfE_R - dfE_F) MSE_F}.$$

```{r}

# Wyliczenia ręczne dla modelu R
SSE_R <- sum((GPA - est_GPA_R)^2)
SST_R <- sum((GPA - mean(GPA))^2)
SSM_R <- SST_R - SSE_R
dfE_R <- n - p_R
dfM_R <- p_R - 1
MSE_R <- SSE_R/dfE_R 
MSM_R <- SSM_R/dfM_R

# Wyliczenia ręczne dla modelu F
SSE_F <- sum((GPA - est_GPA_F)^2)
SST_F <- SST_R
SSM_F <- SST_F - SSE_F
dfE_F <- n - p_F
dfM_F <- p_F - 1
MSE_F <- SSE_F/dfE_F 
MSM_F <- SSM_F/dfM_F

# różnica między SSE z modelu R i F
differ <- SSE_R - SSE_F 

stat_F <- (differ/(dfE_R - dfE_F))/MSE_F
```

Różnicę między SSE obu modeli jest równa `r differ`, natomiast statystyka $F =$ `r stat_F`.

### b)

Dla tego samego testu bierzemy statystykę F, wykorzystując ```anova```.

```{r message=FALSE, warning=FALSE}
anova_R <- anova(model_R)
anova_F <- anova(model_F)

anova_SSE_F <- anova_F$`Sum Sq`[p_F] # SSE wyliczone z ANOVA z R dla modelu F
anova_SSE_R <- anova_R$`Sum Sq`[p_R] # SSE wyliczone z ANOVA z R dla modelu R
anova_MSE_F <- anova_SSE_F/anova_F$Df[p_F]

anova_stat_F <- ((anova_SSE_R - anova_SSE_F)/
                   (anova_R$Df[p_R] - anova_F$Df[p_F]))/anova_MSE_F
```

Wartość statystyki F z wykorzystaniem anova wynosi `r anova_stat_F`. Widzimy, że wyniki jakie otrzmaliśmy są takie same. Teraz wyliczamy kwantyl z rozkładu Fishera-Snedecora, z liczbą stopni swobody `r anova_R$Df[p_R] - anova_F$Df[p_F]` i `r anova_F$Df[p_F]`, a następnie porównujemy z otrzymaną statystyką.

```{r}
fc = qf(0.95, anova_R$Df[p_R] - anova_F$Df[p_F], anova_F$Df[p_F])

anova_stat_F > fc
```

**Wniosek:** Nie mamy podstaw aby odrzucić hipotezę zerową. Oznacza to, że nadal nie wiemy cze zmienne $SATV$ i $SATM$ są istotne w naszym modelu.

## Zadanie 6

Anova zwraca sumy kwadratów typu 1, natomiast sumy kwadratów typu 2 możemy wyliczać ręcznie możemy to liczyć 
ręcznie ze wzoru. Różnica między typami wygląda tak, jak na przyladzie poniżej.

Suma II typu: 
            
              SSM(Xi| X1, ..., X(i-1), X(i+1),...,Xn)

Suma I typu: 
              
              SSM(X1)

              SSM(X2|X1)
              
              SSM(X3|X2, X1)
              
              (...)


Za pomocą odpowiednich przekształceń dostajemy tabelę sum kwadratów 1 i 2 typu:

```{r echo=FALSE}
model_HSS <- lm(GPA ~ SATM + SATV + HSM + HSE + HSS)
model_HSE <- lm(GPA ~ HSS + SATM + SATV + HSM + HSE)
model_HSM <- lm(GPA ~ HSE + HSS + SATM + SATV + HSM)
model_SATV <- lm(GPA ~ HSM + HSE + HSS + SATM + SATV)
model_SATM <- lm(GPA ~ SATV + HSM + HSE + HSS + SATM)

anova_HSS <- anova(model_HSS)
anova_HSE <- anova(model_HSE)
anova_HSM <- anova(model_HSM)
anova_SATV <- anova(model_SATV)
anova_SATM <- anova(model_SATM)

`typ 1` <- round(anova_HSS$`Sum Sq`[1:5], 5) 
typ_2_HSS <- anova_HSS$`Sum Sq`[5]
typ_2_HSE <- anova_HSE$`Sum Sq`[5]
typ_2_HSM <- anova_HSM$`Sum Sq`[5]
typ_2_SATV <- anova_SATV$`Sum Sq`[5]
typ_2_SATM <- anova_SATM$`Sum Sq`[5]
`typ 2` <- round(c(typ_2_SATM, typ_2_SATV, typ_2_HSM, typ_2_HSE, typ_2_HSS),5)
SS <- data.frame(`typ 1`, `typ 2`)
rownames(SS) = c("SATM", "SATV", "HSM", "HSE", "HSS")
SS
```

### a)

Tworzymy dwa modele regresji jeden ze zmiennymi $SATM$, $SATV$, $HSM$, a drugi bez HSM. Chcemy sprawdzić czy różnica sum kwadratów dla obu modeli da nam różnicę sumy kwadratów typu 1 dla $HSM$.

```{r}

# a)
model_2 <- lm(GPA ~ SATM + SATV)
model_3 <- lm(GPA ~ SATM + SATV + HSM)

a2 <- anova(model_2)
a3 <- anova(model_3)
a2
a3
```
```{r echo=FALSE}
SSE_a3 <- a3$`Sum Sq`[4]
SSE_a2 <- a2$`Sum Sq`[3] 
SSE_HSM <- a3$`Sum Sq`[3]

porównanie <- data.frame(SSE_HSM, SSE_a2 - SSE_a3, round(SSE_HSM, 6) == round(SSE_a2 - SSE_a3, 6))
colnames(porównanie) = c("SSM(HSM|SATM, SATV)", "SSM(F) - SSM(R)", "równe" )
porównanie
```

**Wnioski:** Udało nam się pokazać, że różnica SSE modelu zredukowanego i modelu pełnego daje SSE zmiennej, która występowała tylko w jednym z nich. Analogicznie działa to, gdy mówimy o SSM, co wynika z tw. Pitagorasa.

### b)

**Wniosek:** Predyktory dla których wartości SS1 i SS2 są takie same istnieją i są to zawsze ostatnie te które leżą w ostatniej kolumnie macierzy planu. Z definicji SS1 i SS2 wynika, że wyrażone są tym samym wzorem. 

## Zadanie 7

Chcemy zbadać co się stanie, gdyby jedna zmienna w modelu była kombinacją liniową pozostałych. W tym celu tworzymy model, w którym zmiennymi objaśniającymi są $SATV$, $SATM$ oraz $SAT = SATM + SATV$.

```{r echo=FALSE}
SAT <- SATM + SATV
model_SSS <- lm(GPA ~ SATM + SATV + SAT)

summary(model_SSS)
```

Dla jednej ze zmiennych (z ostatniej kolumny macierzy planu) zawsze wychodziło `Na` (niezależnie od kolejności występowania zmiennych w modelu). Może być to spowodowane tym, że zmienna jest kombinacją liniową dwóch poprzednich kolumn i prawdopodobnie R sam wyklucza ją z modelu. 

Jeżeli stworzymy model bez zmiennej $SAT$, otrzymujemy:

```{r echo=FALSE}
SAT <- SATM + SATV
model_SSS <- lm(GPA ~ SATM + SATV)

summary(model_SSS)
```

Widzimy, że wyniki jakie otrzymaliśmy są takie same jak w modelu powyżej. 

**Wniosek:** Jeżeli zmienna niezależna jest kombinacja liniową pozostałych, to R nie uwzględni jej w modelu. Stworzy on jednak regresję bez tej zmiennej i zaznaczy za pomocą `Na`, że nie występuje ona w modelu.
   
## Zadanie 8

```{r echo=FALSE}
model_8F <- lm(GPA ~ HSM + HSE + HSS + SATM + SATV + SEX)
summ_8 <- summary(model_8F)
coef_8 = round(coefficients(model_8F), 5)
```

W tym i następnych zadaniach analizujemy pełny model, którego równanie regresji jest postaci:

$\hat{Y} =$ `r abs(coef_8[1])` - `r abs(coef_8[2])`$X_1$ + `r abs(coef_8[3])`$X_2$ + `r abs(coef_8[4])`$X_3$ + `r abs(coef_8[5])`$X_4$ + `r abs(coef_8[6])`$X_5$ + `r abs(coef_8[7])`$X_6$,

gdzie {X_i} to kolejno HSM, HSE, HSS, SATM, SATV, SEX. 

Wektor residuów opisuje, to czego nie wyjaśniły zmienne objaśniające, zatem cząstkowy wykres rozrzutu (dla zmiennej $X_i$) opisuje relacje między $X_i$, a $Y$ po uwzględnieniu wpływu pozostałych zmiennych niezależnych. Przykłady:

1. Jeżeli na wykresie nie obserwowana jest żadna wyrażna struktur, oznacza to,że zmienna $X_i$ nie wnosi do modelu istotnej informacji ponad to co wniosły pozostałe $X$.

2. Jeżeli obserwujemy relację liniową (rosnącą/malejącą) oznacza to, że zmienna wnosi dodatkową informację do modelu.

3. Możemy szukać odstępstw od założeń modelu, np. obserwacje odstające, brak liniowości, brak stałości wariancji.

### Partial regression plots

```{r echo=FALSE}
#Partial regression plots

par(mfrow=c(1,2))

# dla HSM
model_8F_bez_HSM <- lm(GPA ~ HSE + HSS + SATM + SATV + SEX)
model_8HSM <- lm(HSM ~ HSE + HSS + SATM + SATV + SEX)
plot(residuals(model_8F_bez_HSM) ~ residuals(model_8HSM), 
     xlab = "e(HSM)", ylab = "e(Y)")

# dla HSE
model_8F_bez_HSE <- lm(GPA ~ HSM + HSS + SATM + SATV + SEX)
model_8HSE <- lm(HSE ~ HSM + HSS + SATM + SATV + SEX)
plot(residuals(model_8F_bez_HSE) ~ residuals(model_8HSE),
     xlab = "e(HSE)", ylab = "e(Y)")

# dla HSS
model_8F_bez_HSS <- lm(GPA ~ HSM + HSE + SATM + SATV + SEX)
model_8HSS <- lm(HSS ~ HSM + HSE + SATM + SATV + SEX)
plot(residuals(model_8F_bez_HSS) ~ residuals(model_8HSS),
     xlab = "e(HSS)", ylab = "e(Y)")

# dla SATM
model_8F_bezSATM <- lm(GPA ~ HSM + HSE + HSS + SATV + SEX)
model_8SATM <- lm(SATM ~ HSM + HSE + HSS + SATV + SEX)
plot(residuals(model_8F_bezSATM) ~ residuals(model_8SATM),
     xlab = "e(SATM)", ylab = "e(Y)")

# dla SATV
model_8F_bezSATV <- lm(GPA ~ HSM + HSE + HSS + SATM + SEX)
model_8SATV <- lm(SATV ~ HSM + HSE + HSS + SATM + SEX)
plot(residuals(model_8F_bezSATV) ~ residuals(model_8SATV),
     xlab = "e(SATV)", ylab = "e(Y)")

# dla SEX
model_8F_bezSEX <- lm(GPA ~ HSM + HSE + HSS + SATM + SATV)
model_8SEX <- lm(SEX ~ HSM + HSE + HSS + SATM + SATV)
plot(residuals(model_8F_bezSEX) ~ residuals(model_8SEX),
     xlab = "e(SEX)", ylab = "e(Y)")
```

**Wniosek:** Na wykresach nie obserwujemy, aby zmienne wnosiły dodatkowe informacje, ponad te co już zostały wprowadzone do modelu.

## Zadanie 9

Badamy studentyzowane residua zewnętrzne dla modelu pełnego z zadania 8.

```{r echo=FALSE}
r_zwn <- rstudent((model_8F)) #studentyzacja residuów zewnetrzna
r_wwn <- rstandard(model_8F)
olsrr::ols_plot_resid_stud_fit(model_8F)
outliers <- boxplot.stats(r_zwn)$out
```

**Wniosek:** Na wykresie na czerwono zaznaczone mamy wartości odstające, których jest dokładnie `r length(outliers)` i  wymagają one dodatkowego badania.

## Zadanie 10

Miara DFFITS dla i–tej obserwacji jest standaryzowaną różnicą pomiędzy predykcjami wartości $Y_i$ uzyskanymi na podstawie dwóch modeli skonstruowanych na danych, odpowiednio, z/bez obserwacji $Y_i$. Oczekujemy, że wartości jakie uzyskamy dla obu modeli będą sobie bardzo bliskie, w przeciwnej sytuacji należy im się dokładniej się przyjżeć.

```{r echo=FALSE}
D_dffits <- dffits(model_8F)
big_D_dffits <- which(abs((D_dffits))>2*sqrt(6/224))
`Obserwacje` <- c(1:length(D_dffits))
ggplot() +
  geom_point(data.frame(`Obserwacje`, D_dffits), 
             mapping = aes(x = `Obserwacje`, y = D_dffits))+ 
  geom_line(data.frame(`Obserwacje`,2*sqrt(6/224)), mapping = aes(x = `Obserwacje`, y = 2*sqrt(6/224)), colour = 'red') +
  geom_line(data.frame(`Obserwacje`,2*sqrt(6/224)), mapping = aes(x = `Obserwacje`, y = -2*sqrt(6/224)), colour = 'red')

               
```

**Wnioski:** Obserwację uznajemy za wpływową, jeśli $DFFITS_i>2\sqrt{p/n}$. W naszym przypadku mamy `r length(big_D_dffits)` takich obserwacji. Wymagają one dodatkowej analizy.

## Zadanie 11

Do badania wielkości zjawiska multikolinearności można posłużyć się tzw. miarą Variance inflation factor (VIF). VIF dla k-tej zmiennej objaśniającej bada, w jakim stopniu zmienna $X_k$ objaśniana jest przez pozostałe zmienne objaśniające $X_1, ..., X_{k−1}, X_{k+1}, ..., X_{p−1}$. Duże wartości $VIF_k$ wskazują na bardzo silną korelację między $X_k$ i pewną kombinacją liniową pozostałych zmiennych objaśniających. Implikuje to występowanie zjawiska multikolinearności. Zamiast Variance inflation factor można stosować miarę zwaną Tolerancją, zdefiniowaną jako odwrtność VIF. W przypadku tolerancji na problem z multikolinearnością wskazywać będą wartości mniejsze od $0.1$.

```{r echo=FALSE}
1/vif(model_8F)
```

**Wnioski:** Nie ma problemów z multikolinearnościom zmiennych.

## Zadanie 12 

W tym zadaniu badamy wartości statystyk $AIC$ oraz $BIC$ dla niektórych podzbiorów wyżej analizowanego modelu. Kryteria AIC oraz BIC są stosujemy w celu znalezienia balansu pomiędzy dopasowaniem modelu do danych i nadmierną złożonością modelu. Wyniki dla obu kryteriów przedstawione zostały w tabeli poniżej.

```{r echo=FALSE}
`model HSM` = lm(GPA ~ HSM)
`model HSM + HSS`= lm(GPA ~ HSM + HSS)
`model HSM + HSS + HSE` = lm(GPA ~ HSM + HSS + HSE)
`model HSM + HSS + HSE + SATM`= lm(GPA ~ HSM + HSS + HSE + SATM)
`model HSM + HSS + HSE + SATM + SATV` = lm(GPA ~ HSM + HSS + HSE + SATM + SATV)
`model HSM + HSS + HSE + SATM + SATV + SEX` = lm(GPA ~ HSM + HSS + HSE + SATM + SATV + SEX)

`model SEX` = lm(GPA ~ SEX)
`model SEX + SATV` = lm(GPA ~ SEX + SATV)
`model SEX + SATV + SATM` = lm(GPA ~ SEX + SATV + SATM)
`model SEX + SATV + SATM + HSE` = lm(GPA ~ SEX + SATV + SATM + HSE)
`model SEX + SATV + SATM + HSE + HSS` = lm(GPA ~ SEX + SATV + SATM + HSE + HSS)
`model SEX + SATV + SATM + HSE + HSS + HSM` = lm(GPA ~ SEX + SATV + SATM + HSE + HSS + HSM)
x = AIC(`model HSM`, `model HSM + HSS`, `model HSM + HSS + HSE`, 
        `model HSM + HSS + HSE + SATM`, `model HSM + HSS + HSE + SATM + SATV`, 
        `model HSM + HSS + HSE + SATM + SATV + SEX`,
        `model SEX`, `model SEX + SATV`, `model SEX + SATV + SATM`, 
        `model SEX + SATV + SATM + HSE`, `model SEX + SATV + SATM + HSE + HSS`, 
        `model SEX + SATV + SATM + HSE + HSS + HSM`)
y = BIC(`model HSM`, `model HSM + HSS`, `model HSM + HSS + HSE`, 
        `model HSM + HSS + HSE + SATM`, `model HSM + HSS + HSE + SATM + SATV`, 
        `model HSM + HSS + HSE + SATM + SATV + SEX`,
        `model SEX`, `model SEX + SATV`, `model SEX + SATV + SATM`, 
        `model SEX + SATV + SATM + HSE`, `model SEX + SATV + SATM + HSE + HSS`, 
        `model SEX + SATV + SATM + HSE + HSS + HSM`)

xy_dt = data.frame(cbind(x[2],y[2]))
xy_dt
```

**Wnioski:** Statystyka BIC przyjmuje najmniejszą wartość dla modelu uwzględniającego tylko zmienną $HSM$. Według tego kryterium powinniśmy wybrać właśnie ten model. Kryterium AIC osiąga najmniejszą wartość dla modelu bazującego na HSM oraz HSS. Jednak wartość AIC dla modelu uwzględniającego jedynie $HSM$ jest również bardzo bliska tej wartości. Biorąc pod uwagę oba te kryteria, najlepszy model z podanych jaki możemy wybrać to model regresji prostej uwzględniającej $HSM$.
